{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MavClarito/Emerging-Tech-1/blob/main/Clarito_CPE018_MidtermExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d44fc7c-ab9b-43cc-919b-020cf9fce309",
      "metadata": {
        "id": "1d44fc7c-ab9b-43cc-919b-020cf9fce309"
      },
      "source": [
        "# CPE018 Midterm Exam (1st Sem, A.Y. 2023-2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c73f82-2df6-45d1-852f-70f4f7fe0b73",
      "metadata": {
        "id": "f2c73f82-2df6-45d1-852f-70f4f7fe0b73"
      },
      "source": [
        "Student Submission Details:\n",
        "* Name: Vincent Maverick D. Clarito\n",
        "* Section: CPE31S2\n",
        "* Schedule: 11/17/23\n",
        "* Instructor: Engr. Roman M. Richard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b600f8-951c-4920-873f-ef9ab278fbf0",
      "metadata": {
        "id": "67b600f8-951c-4920-873f-ef9ab278fbf0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fed455-ea33-4e13-a725-0cd25e99041b",
      "metadata": {
        "id": "a3fed455-ea33-4e13-a725-0cd25e99041b"
      },
      "source": [
        "## Intended Learning Outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190f5a5b-4ac1-40f8-9b24-d244e929a3d3",
      "metadata": {
        "id": "190f5a5b-4ac1-40f8-9b24-d244e929a3d3"
      },
      "source": [
        "By the end of this activity, the student should be able to:\n",
        "* ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
        "* ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55584754-3ca5-4a19-83db-17ac39101610",
      "metadata": {
        "id": "55584754-3ca5-4a19-83db-17ac39101610"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9993c5-0f71-4bdc-8f4b-af50975b9d91",
      "metadata": {
        "id": "8d9993c5-0f71-4bdc-8f4b-af50975b9d91"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8324f2-2f10-4a8b-873f-400c32338886",
      "metadata": {
        "id": "7c8324f2-2f10-4a8b-873f-400c32338886"
      },
      "source": [
        "For this examination, you must create a **mood detection** program with an object-oriented programming approach (same as project CAMEO), it must detect mood changes through the use of algorithms/techniques/schemes learned in class, and from external sources.\n",
        "\n",
        "In this file, you have to include for each section of your solution your completion of the following:\n",
        "\n",
        "* Part 1: **Face Detection**: Once your face is detected using any algorithm, it must draw an ROI. The color for the ROI is your choice; however, it must detect for all faces in the frame and draw a corresponding ROI.\n",
        "* Part 2: **Face Recognition**: The detected face must then be recognized, using any of the provided tools in class, the ROIs must indicate whether it is your face or someone it doesn't recognize.\n",
        "* Part 3: **Mood Detection**: Use three different feature detection and matching techniques to determine three emotion: happy, sad and neutral. Two of the techniques must be learned from class, and 1 must be one you independently learned.\n",
        "\n",
        "Properly show through your notebook the output for each part of the exam."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c708ca1c-ed32-40fb-97bc-a017b4380692",
      "metadata": {
        "id": "c708ca1c-ed32-40fb-97bc-a017b4380692"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b3b1fb-327f-4723-aea3-543f1221c683",
      "metadata": {
        "id": "d4b3b1fb-327f-4723-aea3-543f1221c683"
      },
      "source": [
        "## Procedure and Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef60d7f-d688-4ead-b9db-36b3739cc3ff",
      "metadata": {
        "id": "fef60d7f-d688-4ead-b9db-36b3739cc3ff"
      },
      "source": [
        "Notes:\n",
        "* This is the section where you have to include all  your answers to the items provided in the tasks section.\n",
        "* Tasks 1 and 2 contribute directly to ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
        "* Task 3 contributes directly to ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dde1091-4711-4343-a48d-edad768437dd",
      "metadata": {
        "id": "5dde1091-4711-4343-a48d-edad768437dd"
      },
      "source": [
        "### Task 1: Face Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13054d3d-fc5e-4e43-bdac-2f699165dda8",
      "metadata": {
        "id": "13054d3d-fc5e-4e43-bdac-2f699165dda8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 16 11:37:02 2023\n",
        "\n",
        "@author: Mav\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import threading\n",
        "\n",
        "#I used a multithreading where it seperates the video capture and processes\n",
        "# in order for my computer to have less lag.\n",
        "class VideoStream:\n",
        "    def __init__(self, src=0):\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "        (self.grabbed, self.frame) = self.stream.read()\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        threading.Thread(target=self.update, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def update(self):\n",
        "        while not self.stopped:\n",
        "            (self.grabbed, self.frame) = self.stream.read()\n",
        "\n",
        "    def read(self):\n",
        "        return self.frame\n",
        "\n",
        "    def stop(self):\n",
        "        self.stopped = True\n",
        "\n",
        "def detect():\n",
        "    face_cascade = cv2.CascadeClassifier(\n",
        "   'C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/haarcascade_frontalface_default.xml')\n",
        "\n",
        "    vs = VideoStream().start()\n",
        "\n",
        "    while True:\n",
        "        frame = vs.read()\n",
        "        small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
        "        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        " #Scalability of the x,y coordinates as well as the width and height of the drawn rectangle\n",
        "            x *= 2\n",
        "            y *= 2\n",
        "            w *= 2\n",
        "            h *= 2\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "            roi_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "        cv2.imshow(\"camera\", frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    vs.stop()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    detect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc3cfb0",
      "metadata": {
        "id": "3bc3cfb0"
      },
      "outputs": [],
      "source": [
        "#Reference: https://stackoverflow.com/questions/67567464/multi-threading-in-image-processing-video-python-opencv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd46e7b-4155-47de-9ada-a775afb39b2d",
      "metadata": {
        "id": "7fd46e7b-4155-47de-9ada-a775afb39b2d"
      },
      "source": [
        "### Task 2: Face Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de71ae16-2dd0-4171-8a69-4129338d695d",
      "metadata": {
        "id": "de71ae16-2dd0-4171-8a69-4129338d695d"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 16 11:45:07 2023\n",
        "\n",
        "@author: Mav\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "haarcascade = 'C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/haarcascade_frontalface_default.xml'\n",
        "path = 'C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/Mav-FR/Mav-Resized'\n",
        "\n",
        "\n",
        "def read_images(path, sz=None):\n",
        "    c = 0\n",
        "    X, y = [], []\n",
        "\n",
        "    for dirname, dirnames, filenames in os.walk(path):\n",
        "        for subdirname in dirnames:\n",
        "            subject_path = os.path.join(dirname, subdirname)\n",
        "            for filename in os.listdir(subject_path):\n",
        "                try:\n",
        "                    if filename == \".directory\":\n",
        "                        continue\n",
        "                    filepath = os.path.join(subject_path, filename)\n",
        "                    im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                    if sz is not None:\n",
        "                        im = cv2.resize(im, (200, 200))\n",
        "\n",
        "                    X.append(np.asarray(im, dtype=np.uint8))\n",
        "                    y.append(c)\n",
        "\n",
        "                except IOError as e:\n",
        "                    print(f\"I/O Error({e.errno}): {e.strerror}\")\n",
        "                except:\n",
        "                    print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "                    raise\n",
        "            c = c + 1\n",
        "    return [X, y]\n",
        "\n",
        "def face_rec():\n",
        "    names = ['Unrecognized', 'Mav', 'MarkZ']\n",
        "\n",
        "    [X, y] = read_images(path)\n",
        "    y = np.asarray(y, dtype=np.int32)\n",
        "\n",
        "    model = cv2.face.EigenFaceRecognizer_create()\n",
        "    model.train(X, y)\n",
        "\n",
        "    camera = cv2.VideoCapture(0)\n",
        "    face_cascade = cv2.CascadeClassifier(haarcascade)\n",
        "\n",
        "    while True:\n",
        "        ret, img = camera.read()\n",
        "        if not ret:\n",
        "            print(\"Error reading frame.\")\n",
        "            break\n",
        "\n",
        "        faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "            gray = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
        "            roi = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            try:\n",
        "                params = model.predict(roi)\n",
        "                label = names[params[0]]\n",
        "                cv2.putText(img, label + \", \" + str(params[1]), (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        cv2.imshow(\"camera\", img)\n",
        "        if cv2.waitKey(30) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    camera.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    face_rec()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e5b345-0442-4e42-8500-a247a520383e",
      "metadata": {
        "id": "61e5b345-0442-4e42-8500-a247a520383e"
      },
      "source": [
        "### Task 3: Mood Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc361ca0-d355-4470-9fba-2b3b67bb240b",
      "metadata": {
        "id": "dc361ca0-d355-4470-9fba-2b3b67bb240b"
      },
      "outputs": [],
      "source": [
        "# BRIEF Algorithm w/ Face recognition and Detection (1st Technique)\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 16 20:35:08 2023\n",
        "\n",
        "@author: Mav\n",
        "\"\"\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "happy_images = load_images_from_folder(\"C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/Mav-MD/Happy/pgm resized\")\n",
        "neutral_images = load_images_from_folder(\"C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/Mav-MD/Neutral/pgm resized\")\n",
        "sad_images = load_images_from_folder(\"C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/Mav-MD/Sad/pgm resized\")\n",
        "\n",
        "print(\"Number of Happy Images:\", len(happy_images))\n",
        "print(\"Number of Neutral Images:\", len(neutral_images))\n",
        "print(\"Number of Sad Images:\", len(sad_images))\n",
        "\n",
        "brisk = cv2.BRISK_create()\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"Error reading frame.\")\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    print(\"Number of Faces Detected:\", len(faces))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi = gray[y:y + h, x:x + w]\n",
        "        keypoints, descriptors = brisk.detectAndCompute(roi, None)\n",
        "\n",
        "        if descriptors is not None:\n",
        "            descriptors = descriptors.astype(np.float32)\n",
        "        else:\n",
        "            continue  # Skip the current face\n",
        "\n",
        "        match_scores = []\n",
        "        for mood_images, mood_label in zip([happy_images, neutral_images, sad_images],\n",
        "                                           [\"Happy\", \"Neutral\", \"Sad\"]):\n",
        "            mood_match = 0\n",
        "\n",
        "            for img in mood_images:\n",
        "                kp_img, des_img = brisk.detectAndCompute(img, None)\n",
        "\n",
        "                if des_img is None:\n",
        "                    continue\n",
        "\n",
        "                des_img = des_img.astype(np.float32)\n",
        "                bf = cv2.BFMatcher()\n",
        "                matches = bf.knnMatch(descriptors, des_img, k=2)\n",
        "                good_matches = []\n",
        "                for m, n in matches:\n",
        "                    if m.distance < 0.75 * n.distance:\n",
        "                        good_matches.append(m)\n",
        "\n",
        "                mood_match += len(good_matches)\n",
        "\n",
        "            match_scores.append((mood_match, mood_label))\n",
        "\n",
        "        #This chooses the mood with the highest match or that matched my camera\n",
        "        best_match = max(match_scores, key=lambda x: x[0])\n",
        "        mood = best_match[1]\n",
        "\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, mood, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6812dc65",
      "metadata": {
        "id": "6812dc65"
      },
      "outputs": [],
      "source": [
        "#Reference: https://pypi.org/project/face-recognition/l;o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb965e0c-a349-4e1c-b51d-e21db970dc7a",
      "metadata": {
        "id": "eb965e0c-a349-4e1c-b51d-e21db970dc7a"
      },
      "outputs": [],
      "source": [
        "# Use of Haarcascade.smile for mood detection (2nd Technique)\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 16 19:35:42 2023\n",
        "\n",
        "@author: Mav\n",
        "\"\"\"\n",
        "#Technique 2 using smile haarcascade\n",
        "import cv2\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
        "\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = camera.read()\n",
        "    if not ret:\n",
        "        print(\"Error reading frame.\")\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    print(\"Number of Faces Detected:\", len(faces))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        roi_color = frame[y:y + h, x:x + w]\n",
        "\n",
        "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)\n",
        "\n",
        "        # Determine mood based on smile detection\n",
        "        if len(smiles) > 0:\n",
        "            mood = \"Happy\"\n",
        "        else:\n",
        "            mood = \"Neutral\"\n",
        "\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, mood, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46261895-0b3f-4cfa-b62e-46871a91c17b",
      "metadata": {
        "id": "46261895-0b3f-4cfa-b62e-46871a91c17b"
      },
      "outputs": [],
      "source": [
        "# Mood detection with deep learning (Technique 3)\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Nov 17 12:42:51 2023\n",
        "\n",
        "@author: Mav\n",
        "\"\"\"\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "TRAIN_DIR = 'data/train'\n",
        "VAL_DIR = 'data/test'\n",
        "EMOTION_DICT = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "# Check if the directories exist, create them if not\n",
        "for directory in [TRAIN_DIR, VAL_DIR]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# Data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Training and validation generators\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=64,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=64,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# Load trained model\n",
        "model.load_weights('C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/MD/model.h5')\n",
        "\n",
        "# Webcam feed\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Capture frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    facecasc = cv2.CascadeClassifier('C:/Users/Mav/Documents/Midterm Exam - Emerging Tech/haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "        prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(prediction))\n",
        "        cv2.putText(frame, EMOTION_DICT[maxindex], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Video', cv2.resize(frame, (500, 500), interpolation=cv2.INTER_CUBIC))\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cba9401",
      "metadata": {
        "id": "4cba9401"
      },
      "outputs": [],
      "source": [
        "#Reference: https://github.com/travistangvh/emotion-detection-in-real-time\n",
        "#https://github.com/saranshbht/Emotion-detection/blob/master/src/.ipynb_checkpoints/emotions-checkpoint.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b537d4-927d-4adf-84e2-3fe4a7fef365",
      "metadata": {
        "id": "76b537d4-927d-4adf-84e2-3fe4a7fef365"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8c97cb-2d49-4de4-9067-04245f9ab9cf",
      "metadata": {
        "id": "9c8c97cb-2d49-4de4-9067-04245f9ab9cf"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544c6947-908c-4dd2-89c7-7dc4bb2f58f8",
      "metadata": {
        "id": "544c6947-908c-4dd2-89c7-7dc4bb2f58f8"
      },
      "source": [
        "For the three different techniques you used in face detection, provide an in-depth analysis.\n",
        "\n",
        "To do this, you must:\n",
        "* Test the face detection, face recongition, and mood detection functions 10 times each. Only the mood detection will have components for 10 tests for each different technique used.\n",
        "* Create a table containing the 10 tests (like shown below) for each task.\n",
        "* Analyze each output by identifying the accuracy and providing your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a46e05b",
      "metadata": {
        "id": "5a46e05b",
        "outputId": "c3f90696-99a1-46ac-9c12-200178d56a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1: Face Detection\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|   Test # |   # Recognized Face | Did it meet your expectation?   |   Score |\n",
            "+==========+=====================+=================================+=========+\n",
            "|        1 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        2 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        3 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        4 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        5 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        6 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        7 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        8 |                   2 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|        9 |                   1 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "|       10 |                   2 | YES                             |       1 |\n",
            "+----------+---------------------+---------------------------------+---------+\n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# Import the module for tabulating the data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a list for content of the table\n",
        "test_results = [\n",
        "    [\"1\", \"1\", \"YES\", 1],\n",
        "    [\"2\", \"1\", \"YES\", 1],\n",
        "    [\"3\", \"1\", \"YES\", 1],\n",
        "    [\"4\", \"1\", \"YES\", 1],\n",
        "    [\"5\", \"1\", \"YES\", 1],\n",
        "    [\"6\", \"1\", \"YES\", 1],\n",
        "    [\"7\", \"1\", \"YES\", 1],\n",
        "    [\"8\", \"2\", \"YES\", 1],\n",
        "    [\"9\", \"1\", \"YES\", 1],\n",
        "    [\"10\", \"2\", \"YES\", 1]\n",
        "\n",
        "]\n",
        "\n",
        "# Create a list for the headers of your table\n",
        "header = [\"Test #\", \"# Recognized Face\",\"Did it meet your expectation?\", \"Score\"]\n",
        "\n",
        "# display table\n",
        "print(\"Task 1: Face Detection\")\n",
        "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate for the accuracy\n",
        "total = 0\n",
        "for i in test_results:\n",
        "    total += i[3]\n",
        "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad02e6c",
      "metadata": {
        "id": "9ad02e6c"
      },
      "source": [
        "* Analysis: The face detection was able to detect all of the faces from my camera therefore I considered this very accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b079aab-6780-455a-9b69-b779028a02a9",
      "metadata": {
        "id": "2b079aab-6780-455a-9b69-b779028a02a9",
        "outputId": "24a99be1-0c6b-412c-d670-d6369eadd128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 2: Facial Recognition\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|   Test # |   Total # of Faces |   # of Faces Recognized |   Score |\n",
            "+==========+====================+=========================+=========+\n",
            "|        1 |                  2 |                       1 |       0 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        2 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        3 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        4 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        5 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        6 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        7 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        8 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|        9 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "|       10 |                  2 |                       2 |       1 |\n",
            "+----------+--------------------+-------------------------+---------+\n",
            "Accuracy:  90.0\n"
          ]
        }
      ],
      "source": [
        "# Import the module for tabulating the data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a list for content of the table\n",
        "test_results = [\n",
        "    [\"1\", \"2\", \"1\", 0],\n",
        "    [\"2\", \"2\", \"2\", 1],\n",
        "    [\"3\", \"2\", \"2\", 1],\n",
        "    [\"4\", \"2\", \"2\", 1],\n",
        "    [\"5\", \"2\", \"2\", 1],\n",
        "    [\"6\", \"2\", \"2\", 1],\n",
        "    [\"7\", \"2\", \"2\", 1],\n",
        "    [\"8\", \"2\", \"2\", 1],\n",
        "    [\"9\", \"2\", \"2\", 1],\n",
        "    [\"10\", \"2\", \"2\", 1]\n",
        "\n",
        "]\n",
        "\n",
        "# Create a list for the headers of your table\n",
        "header = [\"Test #\", \"Total # of Faces\", \"# of Faces Recognized\", \"Score\"]\n",
        "\n",
        "# display table\n",
        "print(\"Task 2: Facial Recognition\")\n",
        "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate for the accuracy\n",
        "total = 0\n",
        "for i in test_results:\n",
        "    total += i[3]\n",
        "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fabaa8f",
      "metadata": {
        "id": "5fabaa8f"
      },
      "source": [
        "* Analysis: The facial recognition was able to recognize most of the faces from my datasets and it should've been 100% if I showed my whole face on my first trial, therefore I considered this very accurate as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc5d60f",
      "metadata": {
        "id": "1bc5d60f",
        "outputId": "0e1a7be8-6463-4ddc-b114-56303a03d72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 3A: Mood Detection using BRIEF w/ facial recognition and detection\n",
            "+----------+------------+----------+---------+\n",
            "|   Test # | Expected   | Actual   |   Score |\n",
            "+==========+============+==========+=========+\n",
            "|        1 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        2 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        3 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        4 | Sad        | Sad      |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        5 | Sad        | Sad      |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        6 | Sad        | Sad      |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        7 | Neutral    | Neutral  |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        8 | Neutral    | Neutral  |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        9 | Neutral    | Neutral  |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|       10 | Sad        | Happy    |       0 |\n",
            "+----------+------------+----------+---------+\n",
            "Accuracy:  90.0\n"
          ]
        }
      ],
      "source": [
        "# Import the module for tabulating the data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a list for content of the table\n",
        "test_results = [\n",
        "    [\"1\", \"Happy\", \"Happy\", 1],\n",
        "    [\"2\", \"Happy\", \"Happy\", 1],\n",
        "    [\"3\", \"Happy\", \"Happy\", 1],\n",
        "    [\"4\", \"Sad\", \"Sad\", 1],\n",
        "    [\"5\", \"Sad\", \"Sad\", 1],\n",
        "    [\"6\", \"Sad\", \"Sad\", 1],\n",
        "    [\"7\", \"Neutral\", \"Neutral\", 1],\n",
        "    [\"8\", \"Neutral\", \"Neutral\", 1],\n",
        "    [\"9\", \"Neutral\", \"Neutral\", 1],\n",
        "    [\"10\", \"Sad\", \"Happy\", 0]\n",
        "\n",
        "]\n",
        "\n",
        "# Create a list for the headers of your table\n",
        "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
        "\n",
        "# display table\n",
        "print(\"Task 3A: Mood Detection using BRIEF w/ facial recognition and detection\")\n",
        "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate for the accuracy\n",
        "total = 0\n",
        "for i in test_results:\n",
        "    total += i[3]\n",
        "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5df864",
      "metadata": {
        "id": "aa5df864"
      },
      "source": [
        "* Analysis: This mood detection would be much more accurate if I have more datasets for each mood. I only inputted 20 each mood which isn't enough to cover all of my facial features on different POV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69e1550",
      "metadata": {
        "id": "e69e1550",
        "outputId": "af5af9b6-7e16-478a-e448-d2af7f705c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 3B: Mood detection using Haarcascade.smile\n",
            "+----------+------------+----------+---------+\n",
            "|   Test # | Expected   | Actual   |   Score |\n",
            "+==========+============+==========+=========+\n",
            "|        1 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        2 | Neutral    | Happy    |       0 |\n",
            "+----------+------------+----------+---------+\n",
            "|        3 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        4 | Sad        | Sad      |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        5 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        6 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        7 | Neutral    | Neutral  |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        8 | Happy      | Happy    |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|        9 | Neutral    | Neutral  |       1 |\n",
            "+----------+------------+----------+---------+\n",
            "|       10 | Happy      | Neutral  |       0 |\n",
            "+----------+------------+----------+---------+\n",
            "Accuracy:  80.0\n"
          ]
        }
      ],
      "source": [
        "# Import the module for tabulating the data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a list for content of the table\n",
        "test_results = [\n",
        "    [\"1\", \"Happy\", \"Happy\", 1],\n",
        "    [\"2\", \"Neutral\", \"Happy\", 0],\n",
        "    [\"3\", \"Happy\", \"Happy\", 1],\n",
        "    [\"4\", \"Sad\", \"Sad\", 1],\n",
        "    [\"5\", \"Happy\", \"Happy\", 1],\n",
        "    [\"6\", \"Happy\", \"Happy\", 1],\n",
        "    [\"7\", \"Neutral\", \"Neutral\", 1],\n",
        "    [\"8\", \"Happy\", \"Happy\", 1],\n",
        "    [\"9\", \"Neutral\", \"Neutral\", 1],\n",
        "    [\"10\", \"Happy\", \"Neutral\", 0]\n",
        "\n",
        "]\n",
        "\n",
        "# Create a list for the headers of your table\n",
        "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
        "\n",
        "# display table\n",
        "print(\"Task 3B: Mood detection using Haarcascade.smile\")\n",
        "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate for the accuracy\n",
        "total = 0\n",
        "for i in test_results:\n",
        "    total += i[3]\n",
        "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786ccfea",
      "metadata": {
        "id": "786ccfea"
      },
      "source": [
        "* Analysis: This is less accurate due to the fact that this only detects smiling person and conditionalized to display \"Neutral or Sad\" when it didn't detect any smile from the camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d9f4f5",
      "metadata": {
        "id": "f2d9f4f5",
        "outputId": "637bafb5-c82d-412a-966d-ac17e0be9b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 3C: Mood detection using deep convolutional neural networks\n",
            "+----------+------------+-----------+---------+\n",
            "|   Test # | Expected   | Actual    |   Score |\n",
            "+==========+============+===========+=========+\n",
            "|        1 | Happy      | Happy     |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        2 | Angry      | Angry     |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        3 | Sad        | Neutral   |       0 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        4 | Surprised  | Surprised |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        5 | Surprised  | Surprised |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        6 | Happy      | Happy     |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        7 | Sad        | Angry     |       0 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        8 | Angry      | Angry     |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|        9 | Sad        | Sad       |       1 |\n",
            "+----------+------------+-----------+---------+\n",
            "|       10 | Sad        | Angry     |       0 |\n",
            "+----------+------------+-----------+---------+\n",
            "Accuracy:  70.0\n"
          ]
        }
      ],
      "source": [
        "# Import the module for tabulating the data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a list for content of the table\n",
        "test_results = [\n",
        "    [\"1\", \"Happy\", \"Happy\", 1],\n",
        "    [\"2\", \"Angry\", \"Angry\", 1],\n",
        "    [\"3\", \"Sad\", \"Neutral\", 0],\n",
        "    [\"4\", \"Surprised\", \"Surprised\", 1],\n",
        "    [\"5\", \"Surprised\", \"Surprised\", 1],\n",
        "    [\"6\", \"Happy\", \"Happy\", 1],\n",
        "    [\"7\", \"Sad\", \"Angry\", 0],\n",
        "    [\"8\", \"Angry\", \"Angry\", 1],\n",
        "    [\"9\", \"Sad\", \"Sad\", 1],\n",
        "    [\"10\", \"Sad\", \"Angry\", 0]\n",
        "\n",
        "]\n",
        "\n",
        "# Create a list for the headers of your table\n",
        "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
        "\n",
        "# display table\n",
        "print(\"Task 3C: Mood detection using deep convolutional neural networks\")\n",
        "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate for the accuracy\n",
        "total = 0\n",
        "for i in test_results:\n",
        "    total += i[3]\n",
        "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30664515",
      "metadata": {
        "id": "30664515"
      },
      "source": [
        "* Analysis: This is very accurate for various moods although the downside of this it wasn't accurate enough to detect sad emotion. This is much better for detecting multiple moods of a person in a live camera feed without taking a picture of each person and their moods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef116e08-3c33-46dc-a5bd-613a50155f73",
      "metadata": {
        "id": "ef116e08-3c33-46dc-a5bd-613a50155f73"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfcc148-9c95-4125-a25c-965f16a7260e",
      "metadata": {
        "id": "cdfcc148-9c95-4125-a25c-965f16a7260e"
      },
      "source": [
        "## Summary and Lessons Learned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e234d297-014b-4ab6-a427-627cf25b7632",
      "metadata": {
        "id": "e234d297-014b-4ab6-a427-627cf25b7632"
      },
      "source": [
        "I have concluded and learned that environmental factors, objects, and facial features are factors that meddle with the accuracy of facial detection. For example, when using face detection, if you have longer bangs or hair, it doesn't detect your face. Another observation from my trials in facial recognition is that you need to have a high-definition or high-quality raw image for accurate face detection and recognition when it is converted into a grayscale and PGM file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb275b8-d25b-4b4e-b1f3-e7ae48fd9de5",
      "metadata": {
        "id": "0bb275b8-d25b-4b4e-b1f3-e7ae48fd9de5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c9194e-b263-4d38-bfa3-ed6490825a43",
      "metadata": {
        "id": "d9c9194e-b263-4d38-bfa3-ed6490825a43"
      },
      "source": [
        "**Proprietary Clause**\n",
        "\n",
        "Property of the Technological Institute of the Philippines (T.I.P.). No part of the materials made and uploaded in this learning management system by T.I.P. may be copied, photographed, printed, reproduced, shared, transmitted, translated or reduced to any electronic medium or machine-readable form, in whole or in part, without prior consent of T.I.P.\n",
        "\n",
        "Prepared by Engr. RMR"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}